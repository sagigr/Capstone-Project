## Capstone Project: Milestone Report

# Exploratory Data Analysis and Modeling

Sagi Greenstine

## About the Report and the Assignment

This is the Milestone Report of the [Capstone Project](https://www.coursera.org/learn/data-science-project) of Data Science Specialization introduced by Johns Hopkins University through Coursera.  
The main goal of the Capstone Project is creating a data product that can predict the next word that the user will type (predicting the next word based on the previous 1, 2, or 3 words, and building a model to handle cases where people will want to type a combination of words that does not appear in the corpora).  
The goal of this report is just to display that I've gotten used to working with the data and that I'm on track to create my prediction algorithm. The report explains my exploratory analysis and my goals for the eventual app and algorithm.

## Data Processing

### Loading and Preprocessing the Data

The [data set](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) is from a blogs called [HC Corpora](www.corpora.heliohost.org).

In this report I will use the English database, but the data set also includes three other databases in German, Russian and Finnish.  
In the English data set we have the three following files of the English Blogs, News and Twitter data:

- **en_US.blogs.txt** 
- **en_US.news.txt**
- **en_US.twitter.txt**

Let's see some basic details of these files, such as sizes of the files (Mbs):

```{r loaddata, cache=TRUE, warning=FALSE}
file.info("Coursera-Swiftkey/final/en_US/en_US.blogs.txt")$size / 1024^2
file.info("Coursera-Swiftkey/final/en_US/en_US.news.txt")$size / 1024^2
file.info("Coursera-Swiftkey/final/en_US/en_US.twitter.txt")$size / 1024^2
blogs <- readLines("Coursera-Swiftkey/final/en_US/en_US.blogs.txt")
news <- readLines("Coursera-Swiftkey/final/en_US/en_US.news.txt")
twitter <- readLines("Coursera-Swiftkey/final/en_US/en_US.twitter.txt")
```

Let's see some statistics about the data sets, such as:  
1. Lines - number of lines (number of non-missing strings in the vector);  
2. LinesNEmpty - number of lines with at least one non-WHITE_SPACE character;  
3. Chars - total number of Unicode code points detected;  
4. CharsNWhite - number of Unicode code points that are not WHITE_SPACEs;  

```{r statdataset,cache=TRUE, warning=FALSE}
library(stringi)
stri_stats_general(blogs)
stri_stats_general(news)
stri_stats_general(twitter)
```

Now, I want to make the exploratory analysis of the data, including the tokenization and frequencies' demonstration.  
**Note:** Due to the enormous amount of time and memory required to analyze the full data sets, I will use the samples of the data files with 10,000 lines of each data set.

```{r samples, ,cache=TRUE, warning=FALSE}
blogsample <- blogs[1:10000]
newsample <- news[1:10000]
twittersample <- twitter[1:10000]
```

### Explorating the Data: Tokenization and Frequencies

#### Tokenization the Data

For tokenization the data, I will clean the data by removing:  
- the numbers;  
- the white spaces;  
- the special characters;  
- the profanity words (I used the text file from this source: [bad words](https://github.com/Xephi/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en)).  

I will use the **tm** package for this job.  
After the tokenization, I will show three first lines of each data set: blogs, news and twitter, respectively:

**Blogs**

```{r blogstokenization, echo = FALSE, cache=TRUE, warning=FALSE}
library(tm)
tblogs <- VCorpus(VectorSource(blogsample))
tblogs <- tm_map(tblogs, removeNumbers) 
tblogs <- tm_map(tblogs, stripWhitespace)
tblogs <- tm_map(tblogs, content_transformer(tolower))
tblogs <- tm_map(tblogs, removePunctuation)
badwords <- readLines("./badwords.txt")
tblogs <- tm_map(tblogs, removeWords, badwords)
blogsdata <- data.frame(text=unlist(sapply(tblogs, `[`, "content")), stringsAsFactors=F)
blogsdata[1:3,1]
```

**News**

```{r newstokenization, echo = FALSE, cache=TRUE, warning=FALSE}
tnews <- VCorpus(VectorSource(newsample))
tnews <- tm_map(tnews, removeNumbers) 
tnews <- tm_map(tnews, stripWhitespace)
tnews <- tm_map(tnews, content_transformer(tolower))
tnews <- tm_map(tnews, removePunctuation)
tnews <- tm_map(tnews, removeWords, badwords)
newsdata <- data.frame(text=unlist(sapply(tnews, `[`, "content")), stringsAsFactors=F)
newsdata[1:3,1]
```

**Twitter**

```{r twittertokenization, echo = FALSE, cache=TRUE, warning=FALSE}
ttwitter <- VCorpus(VectorSource(twittersample))
ttwitter <- tm_map(ttwitter, removeNumbers) 
ttwitter <- tm_map(ttwitter, stripWhitespace)
ttwitter <- tm_map(ttwitter, content_transformer(tolower))
ttwitter <- tm_map(ttwitter, removePunctuation)
ttwitter <- tm_map(ttwitter, removeWords, badwords)
twitterdata <- data.frame(text=unlist(sapply(ttwitter, `[`, "content")), stringsAsFactors=F)
twitterdata[1:3,1]
```

#### Frequencies Analysis

I will use the **RWeka** package to create one-, bi- and tri-grams sets:

```{r rweka, cache=TRUE, warning=FALSE}
library(RWeka)
onegramsblogs <- NGramTokenizer(blogsdata, Weka_control(min = 1, max = 1))
bigramsblogs <- NGramTokenizer(blogsdata, Weka_control(min = 2, max = 2))
trigramsblogs <- NGramTokenizer(blogsdata, Weka_control(min = 3, max = 3))

onegramsnews <- NGramTokenizer(newsdata, Weka_control(min = 1, max = 1))
bigramsnews <- NGramTokenizer(newsdata, Weka_control(min = 2, max = 2))
trigramsnews <- NGramTokenizer(newsdata, Weka_control(min = 3, max = 3))

onegramstwit <- NGramTokenizer(twitterdata, Weka_control(min = 1, max = 1))
bigramstwit <- NGramTokenizer(twitterdata, Weka_control(min = 2, max = 2))
trigramstwit <- NGramTokenizer(twitterdata, Weka_control(min = 3, max = 3))
```

In the next step I will sort the n-grams to get the top 10 used n-grams in the each data set.

```{r sortgrams, echo=FALSE, cache=TRUE, warning=FALSE}
onegramsblogstab <- data.frame(table(onegramsblogs))
onegramsnewstab <- data.frame(table(onegramsnews))
onegramstwittab <- data.frame(table(onegramstwit))

bigramsblogstab <- data.frame(table(bigramsblogs))
bigramsnewstab <- data.frame(table(bigramsnews))
bigramstwittab <- data.frame(table(bigramstwit))

trigramsblogstab <- data.frame(table(trigramsblogs))
trigramsnewstab <- data.frame(table(trigramsnews))
trigramstwittab <- data.frame(table(trigramstwit))

onegramsblogssort <- onegramsblogstab[order(onegramsblogstab$Freq,decreasing = TRUE),]
onegramsnewssort <- onegramsnewstab[order(onegramsnewstab$Freq,decreasing = TRUE),]
onegramstwitsort <- onegramstwittab[order(onegramstwittab$Freq,decreasing = TRUE),]

bigramsblogssort <- bigramsblogstab[order(bigramsblogstab$Freq,decreasing = TRUE),]
bigramsnewssort <- bigramsnewstab[order(bigramsnewstab$Freq,decreasing = TRUE),]
bigramstwitsort <- bigramstwittab[order(bigramstwittab$Freq,decreasing = TRUE),]

trigramsblogssort <- trigramsblogstab[order(trigramsblogstab$Freq,decreasing = TRUE),]
trigramsnewssort <- trigramsnewstab[order(trigramsnewstab$Freq,decreasing = TRUE),]
trigramstwitsort <- trigramstwittab[order(trigramstwittab$Freq,decreasing = TRUE),]

top10onegramsblogs <- onegramsblogssort[1:10,]
top10onegramsnews <- onegramsnewssort[1:10,]
top10onegramstwitter <- onegramstwitsort[1:10,]

top10bigramsblogs <- bigramsblogssort[1:10,]
top10bigramsnews <- bigramsnewssort[1:10,]
top10bigramstwitter <- bigramstwitsort[1:10,]

top10trigramsblogs <- trigramsblogssort[1:10,]
top10trigramsnews <- trigramsnewssort[1:10,]
top10trigramstwitter <- trigramstwitsort[1:10,]

```

And after this, let's build the plots of top 10 used n-grams for each data set:

**The top 10 used words in blogs, news and twitter (respectively):**

```{r plots, fig.width=8, fig.height=15, echo=FALSE, cache=TRUE, warning=FALSE}
library(ggplot2)
library(gridExtra)
onegramsblogsplot <- ggplot(top10onegramsblogs, aes(x=onegramsblogs,y=Freq)) + geom_bar(stat="Identity") +geom_text(aes(label=Freq), vjust=-0.4)

onegramsnewsplot <- ggplot(top10onegramsnews, aes(x=onegramsnews,y=Freq)) + geom_bar(stat="Identity") +geom_text(aes(label=Freq), vjust=-0.4)

onegramstwitplot <- ggplot(top10onegramstwitter, aes(x=onegramstwit,y=Freq)) + geom_bar(stat="Identity") +geom_text(aes(label=Freq), vjust=-0.4)

grid.arrange(onegramsblogsplot, onegramsnewsplot, onegramstwitplot)
```

**The top 10 used bi-grams in blogs, news and twitter (respectively):**

```{r biplots, fig.width=8, fig.height=15, echo=FALSE, cache=TRUE, warning=FALSE}
bigramsblogsplot <- ggplot(top10bigramsblogs, aes(x=bigramsblogs,y=Freq)) + geom_bar(stat="Identity") +geom_text(aes(label=Freq), vjust=-0.4)

bigramsnewsplot <- ggplot(top10bigramsnews, aes(x=bigramsnews,y=Freq)) + geom_bar(stat="Identity") +geom_text(aes(label=Freq), vjust=-0.4)

bigramstwitplot <- ggplot(top10bigramstwitter, aes(x=bigramstwit,y=Freq)) + geom_bar(stat="Identity") +geom_text(aes(label=Freq), vjust=-0.4)

grid.arrange(bigramsblogsplot, bigramsnewsplot, bigramstwitplot)
```

**The top 10 used tri-grams in blogs, news and twitter (respectively):**

```{r triplots, fig.width=8, fig.height=15, echo=FALSE, cache=TRUE, warning=FALSE}
trigramsblogsplot <- ggplot(top10trigramsblogs, aes(x=trigramsblogs,y=Freq)) + geom_bar(stat="Identity") +geom_text(aes(label=Freq), vjust=-0.4)+theme(axis.text.x = element_text(angle = 45, hjust = 1))

trigramsnewsplot <- ggplot(top10trigramsnews, aes(x=trigramsnews,y=Freq)) + geom_bar(stat="Identity") +geom_text(aes(label=Freq), vjust=-0.4)+theme(axis.text.x = element_text(angle = 45, hjust = 1))

trigramstwitplot <- ggplot(top10trigramstwitter, aes(x=trigramstwit,y=Freq)) + geom_bar(stat="Identity") +geom_text(aes(label=Freq), vjust=-0.4)+theme(axis.text.x = element_text(angle = 45, hjust = 1))

grid.arrange(trigramsblogsplot, trigramsnewsplot, trigramstwitplot)
```

## My Plans for Creating a Prediction Algorithm and Shiny App

I plan to make even more cleaning and combining data from all the three files, and making a representive sampling, to build a model and an algorithm to predict the next word, implementing it through Shiny App.
